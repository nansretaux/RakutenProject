{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b7e6a-a9a5-44a0-8c46-19c25d476cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image \n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import skimage\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skimage.feature import hog\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "import h5py\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b54e6e-10bf-457e-9173-064c46d20f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un tableau NumPy à partir du fichier HDF5\n",
    "with h5py.File('data_0_comp.h5', 'r') as hf:\n",
    "    X_train_img_id = hf['image_ids'][:]\n",
    "    X_train_product_id = hf['product_ids'][:]\n",
    "    X_train_hog = hf['X_train_hog'][:]\n",
    "    X_train_hsv = hf['X_train_hsv'][:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7c38f-2db8-4f29-8a36-d3dbc0ecbab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b3188c8-df74-43ef-8598-0510be7787ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train_txt = pd.read_csv('text_lemm.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9996ad6-a2bd-48d1-a15b-6d4239556fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on filtre sur les images qui ont bien été traitées\n",
    "df_features = df_X_train_txt[df_X_train_txt['imageid'].isin(X_train_img_id)]\n",
    "y = df_features['prdtypecode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0fc03d9-ff13-4655-a76a-9371aa827dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation of the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "#Adding .values.astype('U') to avoid error np.nan is an invalid document, expected byte or unicode string.\n",
    "X_train_tfidf = vectorizer.fit_transform(df_X_train_txt['Text'].values.astype('U'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "44a53b59-3bcf-4161-90f5-f61ef88c879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer la variable explicative de la variable à prédire\n",
    "X_tfidf, y_tfidf = df_X_train_txt['Text'], y\n",
    "\n",
    "# Séparer le jeu de données en données d'entraînement et données test \n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, y_tfidf, test_size=0.2, random_state = 30)\n",
    "\n",
    "#Importer le package du TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Créer un vectorisateur \n",
    "vec_tfidf = TfidfVectorizer()\n",
    "\n",
    "# Mettre à jour la valeur de X_train_tfidf et X_test_tfidf\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = vec_tfidf.transform(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a38a8a-adec-47a3-a2f4-c8ac28bef1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un classificateur clf_tfidf et entraîner le modèle sur l'ensemble d'entraînement\n",
    "clf_tfidf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Calculer les prédictions \n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f9eb4-ae56-43b7-8219-bb4e60252d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul et affichage de classification_report\n",
    "print( classification_report(y_test_tfidf, y_pred_tfidf) )\n",
    "\n",
    "# Calcul et affichage de la matrice de confusion\n",
    "conf_matrix_tfidf = pd.crosstab(y_test_tfidf, y_pred_tfidf, rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
    "conf_matrix_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b212dc-a48f-4989-bfec-607c958c5704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea3d68-c052-4020-913a-3675c1bb9fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a04a89-17b3-48eb-a97d-f490dc4f580d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500ce13-7bc0-4f75-8c7c-aae55be7bacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bac2d-44dd-4850-9907-18f42b240558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rakuten",
   "language": "python",
   "name": "rakuten"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
