{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74ebb40-8f9e-465b-8f27-da73fe0065fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the package useful for the development \n",
    "import pandas as pd \n",
    "from unidecode import unidecode \n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Loading the 2 CSVs\n",
    "X_train = pd.read_csv('X_train.csv', delimiter=',', index_col=0)\n",
    "y_train = pd.read_csv('Y_train.csv', delimiter=',', index_col=0)\n",
    "\n",
    "# Concatening them to only have 1 DF\n",
    "X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "# Merging the column designation with the description and put everything in lowercase\n",
    "X_train[\"Text\"] = X_train['designation'].fillna('').str.lower() + ' ' + X_train['description'].fillna('').str.lower()\n",
    "\n",
    "# Cleaning the text\n",
    "# Deleting special character and accent with unidecode\n",
    "X_train['Text'] = X_train['Text'].apply(unidecode).astype('str')\n",
    "# Deleting HTML code\n",
    "X_train['Text'] = X_train['Text'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "# Tokenisation et deleting words with less than 3 letters\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z-]{3,}\")\n",
    "X_train['Text'] = X_train['Text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "# Cleaning the DF\n",
    "X_train.drop(['designation', 'description'], axis=1, inplace = True)\n",
    "\n",
    "#Deleting the stop words\n",
    "stop_words = set(stopwords.words(['english','french','german']))\n",
    "# Adding in addition to the stop words, the words useless for us\n",
    "parasite_words_words = ['plus', 'peut', 'etre', 'tout', 'cette', 'tres']\n",
    "html_code_words = ['rsquo', 'eacute', 'agrave', 'egrave', 'div', 'span', 'class', 'nbsp', 'amp', 'ecirc', 'ccedil', 'laquo', 'raquo']\n",
    "stop_words.update(parasite_words_words)\n",
    "stop_words.update(html_code_words)\n",
    "# Function to delete stop words from our DF\n",
    "def stop_words_filtering(mots) :\n",
    "    tokens = []\n",
    "    for mot in mots:\n",
    "        if mot not in stop_words:  \n",
    "            tokens.append(mot)\n",
    "    return tokens\n",
    "#Deleting stop words from our DF using our function\n",
    "X_train[\"Text\"] = X_train[\"Text\"].apply(stop_words_filtering)\n",
    "\n",
    "#Lemmatization using Spacy and french dictionary\n",
    "#nlp = spacy.load('fr_core_news_sm')                 #!python -m spacy download fr_core_news_sm     <- Ã  mettre sous condition\n",
    "#def spacy_lemmatizer(list):        \n",
    "#    text = ' '.join(list)\n",
    "#    doc = nlp(text)\n",
    "#    list_output = [token.lemma_ for token in doc]\n",
    "#    return ' '.join(list_output)\n",
    "#X_train['Text'] = X_train['Text'].apply(spacy_lemmatizer)\n",
    "\n",
    "# Initialiser le lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Fonction pour lemmatiser une liste de tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "X_train['Text'] = X_train['Text'].apply(lemmatize_tokens)\n",
    "\n",
    "X_train['String'] = X_train['Text'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ddd16d-b700-4ee4-81b5-29f771cc37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mettre sous csv\n",
    "X_train.to_csv('text_lemm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d890c-394f-4f13-b67a-fbca39fd84ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d90a089a-f923-499f-b62e-e1e9a2fe01c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>Text</th>\n",
       "      <th>String</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>[olivia, personalisiertes, notizbuch, seiten, ...</td>\n",
       "      <td>olivia personalisiertes notizbuch seiten punkt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>[journal, art, ndeg, art, marche, salon, art, ...</td>\n",
       "      <td>journal art ndeg art marche salon art asiatiqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>[peluche, donald, europe, disneyland, marionne...</td>\n",
       "      <td>peluche donald europe disneyland marionnette d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>[guerre, tuques, luc, grandeur, veut, organise...</td>\n",
       "      <td>guerre tuques luc grandeur veut organiser jeu ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productid     imageid  prdtypecode  \\\n",
       "0  3804725264  1263597046           10   \n",
       "1   436067568  1008141237         2280   \n",
       "2   201115110   938777978           50   \n",
       "3    50418756   457047496         1280   \n",
       "4   278535884  1077757786         2705   \n",
       "\n",
       "                                                Text  \\\n",
       "0  [olivia, personalisiertes, notizbuch, seiten, ...   \n",
       "1  [journal, art, ndeg, art, marche, salon, art, ...   \n",
       "2  [grand, stylet, ergonomique, bleu, gamepad, ni...   \n",
       "3  [peluche, donald, europe, disneyland, marionne...   \n",
       "4  [guerre, tuques, luc, grandeur, veut, organise...   \n",
       "\n",
       "                                              String  \n",
       "0  olivia personalisiertes notizbuch seiten punkt...  \n",
       "1  journal art ndeg art marche salon art asiatiqu...  \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...  \n",
       "3  peluche donald europe disneyland marionnette d...  \n",
       "4  guerre tuques luc grandeur veut organiser jeu ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0973ef6-d6b7-4773-9954-2148fe17d2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rakuten",
   "language": "python",
   "name": "rakuten"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
