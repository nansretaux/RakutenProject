{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4628b970-a65b-4a47-aa6a-439f8f4d168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import h5py\n",
    "\n",
    "from unidecode import unidecode \n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89d8413-41bf-4a73-9f91-e6a691536b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin des fichiers X_train.csv etc\n",
    "dossier_antoine = '/Users/antoine/Documents/DataScientest/Projet/data/'\n",
    "dossier_nans = ''\n",
    "dossier_tristan = '../data/'\n",
    "#...\n",
    "\n",
    "\n",
    "# chemin des images\n",
    "folder_img_antoine = '/Users/antoine/Pictures/DataScientest_Rakuten/original/image_train/'\n",
    "folder_img_nans = 'C:/Users/Nans/OneDrive/Documents/Rakuten Project/images/images/image_train/'\n",
    "folder_img_tristan = '../data/images/images/image_train/'\n",
    "#...\n",
    "\n",
    "\n",
    "dossier = dossier_tristan          # à modifier pour chacun\n",
    "folder_img = folder_img_tristan   # à modifier pour chacun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fae7db-1c9b-4202-b630-c2e525ec531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the 2 CSVs\n",
    "X = pd.read_csv(dossier + 'X_train.csv', delimiter=',', index_col=0)\n",
    "y = pd.read_csv(dossier + 'Y_train.csv', delimiter=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a247d6a4-1b71-46e7-be3c-4dcfbfd268c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met de côté un jeu de test auquel on ne touchera pas jusqu'à la fin, au moment de mesurer la performance du modèle retenu\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b514c09-3d2f-4292-94ce-607237de192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocessing(img_filename):\n",
    "\n",
    "    \"\"\"\n",
    "    - Recadre l'image en lui retirant ses marges blanches si elle en a ;\n",
    "    - Redimensionne l'image en 100 x 100 ;\n",
    "    - Transforme l'image en niveaux de gris.\n",
    "    \"\"\"\n",
    "    \n",
    "    width = 100\n",
    "    height = 100\n",
    "\n",
    "    # Lecture de l'image avec OpenCV\n",
    "    img = cv2.imread(folder_img + img_filename)\n",
    "    \n",
    "    # Conversion de l'image en niveaux de gris\n",
    "    image_gris = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Inversion des niveaux de gris\n",
    "    image_inversee = cv2.bitwise_not(image_gris)\n",
    "\n",
    "    # Recherche des contours dans l'image\n",
    "    contours, hierarchy = cv2.findContours(image_inversee, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        \n",
    "        # Recherche du contour le plus externe\n",
    "        contour_externe = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Recherche des coordonnées du rectangle englobant le contour externe\n",
    "        x_min, y_min, w, h = cv2.boundingRect(contour_externe)\n",
    "\n",
    "        # Rognement de l'image en utilisant les coordonnées du rectangle englobant\n",
    "        cropped_image = image_gris[y_min:y_min+h, x_min:x_min+w]\n",
    "        #Ou pour des images en couleurs :\n",
    "        #cropped_image = img[y_min:y_min+h, x_min:x_min+w]\n",
    "        #cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Redimensionnement de l'image pour qu'elle ait la même taille que les autres\n",
    "        cropped_image_resized = cv2.resize(cropped_image, (width, height))\n",
    "\n",
    "        return np.array(cropped_image_resized)\n",
    "\n",
    "    else:\n",
    "        # Si aucun contour n'est trouvé, on se contente de redimensionner l'image\n",
    "        image_resized = cv2.resize(image_gris, (width, height))\n",
    "        #Ou pour des images en couleurs :\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #image_resized = cv2.resize(img, (width, height))\n",
    "        return np.array(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfdba58-21a3-41e0-8e95-cda8de7e69b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m processed_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_filename \u001b[38;5;129;01min\u001b[39;00m img_filenames:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Recadrage, redimensionnement et transformation en gris (cf. fonction définie plus haut)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimg_preprocessing\u001b[49m(img_filename)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m     10\u001b[0m     processed_images\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "img_filenames = [f\"image_{X.loc[idx, 'imageid']}_product_{X.loc[idx, 'productid']}.jpg\" for idx in X.index]\n",
    "\n",
    "processed_images = []\n",
    "\n",
    "for img_filename in img_filenames:\n",
    "    # Recadrage, redimensionnement et transformation en gris (cf. fonction définie plus haut)\n",
    "    img = img_preprocessing(img_filename)\n",
    "\n",
    "    # \n",
    "    processed_images.append(img)\n",
    "\n",
    "processed_images = np.array(processed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89fb7cfe-7458-4c1f-8c67-a8097833e5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 41.9 s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Tristan: pretraitement des images identique avec rescale et niveau de gris mais plus rapide car avec paralellisation \n",
    "\n",
    "filenames = [f\"image_{X.loc[idx, 'imageid']}_product_{X.loc[idx, 'productid']}.jpg\" for idx in X.index]\n",
    "\n",
    "# Définir une fonction pour traiter une image, size pour taille image et color = 'nb' pour sortie en n&b\n",
    "def process_image(filename, size, color=None):\n",
    "    img_scr = os.path.join(folder_img, filename)\n",
    "    img = cv2.imread(img_scr)\n",
    "    gris = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_inv = cv2.bitwise_not(gris)\n",
    "    contours, hierarchy = cv2.findContours(img_inv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if color == 'nb':\n",
    "        img = gris\n",
    "    # si contours trouvé\n",
    "    if len(contours) > 0:\n",
    "        contour_exterieur = max(contours, key=cv2.contourArea)\n",
    "        # Obtenir les coordonnées du rectangle englobant\n",
    "        x, y, w, h = cv2.boundingRect(contour_exterieur)\n",
    "        # Extraire la région d'intérêt (ROI)\n",
    "        img = img[y:y+h, x:x+w]\n",
    "    return cv2.resize(img, (size, size)) / 255.0\n",
    "\n",
    "# Paralléliser le traitement des images\n",
    "processed_images = Parallel(n_jobs=-1)(delayed(process_image)(filename, 100, color=\"nb\") for filename in filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cd553c-83f6-43ea-a5e4-997d91ee767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_txt = X.loc[:, ['designation', 'description']]\n",
    "\n",
    "# Merging the column designation with the description and put everything in lowercase\n",
    "X_txt[\"text\"] = X_txt['designation'].fillna('').str.lower() + ' ' + X_txt['description'].fillna('').str.lower()\n",
    "\n",
    "# Cleaning the dataframe by dropping the columns which are not useful anymore\n",
    "X_txt.drop(['designation', 'description'], axis=1, inplace = True)\n",
    "\n",
    "# Cleaning the text\n",
    "# Deleting special character and accent with unidecode\n",
    "X_txt['text'] = X_txt['text'].apply(unidecode).astype('str')\n",
    "# Deleting HTML code\n",
    "X_txt['text'] = X_txt['text'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "# Tokenisation et deleting words with less than 3 letters\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z-]{3,}\")\n",
    "X_txt['text'] = X_txt['text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "#Deleting the stop words\n",
    "stop_words = set(stopwords.words(['english','french','german']))\n",
    "# Adding in addition to the stop words, the words useless for us\n",
    "parasite_words_words = ['plus', 'peut', 'etre', 'tout', 'cette', 'tres']\n",
    "html_code_words = ['rsquo', 'eacute', 'agrave', 'egrave', 'div', 'span', 'class', 'nbsp', 'amp', 'ecirc', 'ccedil', 'laquo', 'raquo']\n",
    "stop_words.update(parasite_words_words)\n",
    "stop_words.update(html_code_words)\n",
    "# Function to delete stop words from our DF\n",
    "def stop_words_filtering(mots) :\n",
    "    tokens = []\n",
    "    for mot in mots:\n",
    "        if mot not in stop_words:  \n",
    "            tokens.append(mot)\n",
    "    return tokens\n",
    "#Deleting stop words from our DF using our function\n",
    "X_txt[\"text\"] = X_txt[\"text\"].apply(stop_words_filtering)\n",
    "\n",
    "# Initialiser le lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Fonction pour lemmatiser une liste de tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "X_txt['text'] = X_txt['text'].apply(lemmatize_tokens)\n",
    "X_txt = X_txt['text'].apply(lambda x: ' '.join(x)).astype(str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08feaaaa-70fb-4611-8bc8-330e7a8c400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un fichier HDF5 et y enregistrer un tableau NumPy\n",
    "with h5py.File('processed_data.h5', 'w') as hf:\n",
    "    hf.create_dataset('img', data=processed_images, compression='gzip')\n",
    "    hf.create_dataset('image_id', data=np.array(X['imageid']), compression='gzip')\n",
    "    hf.create_dataset('product_id', data=np.array(X['productid']), compression='gzip')\n",
    "    hf.create_dataset('label', data=np.array(y['prdtypecode']), compression='gzip')\n",
    "    hf.create_dataset('text', data=X_txt, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9909d7-16f8-423a-8f55-b6d25d73b6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
